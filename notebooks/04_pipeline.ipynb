{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e073043e",
   "metadata": {},
   "source": [
    "## <center style=\"color:blue;\">**SparkAttriNet**</center>\n",
    "\n",
    "Dans le secteur bancaire, anticiper la perte de clients est essentiel pour réduire le taux d’attrition et renforcer la fidélisation. \n",
    "\n",
    "Ce projet exploite la puissance de PySpark pour analyser de grands volumes de données, MLlib pour entraîner un modèle prédictif, MongoDB pour stocker les données transformées, et Streamlit pour visualiser les résultats et faciliter la prise de décision."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b86c02",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### <span style=\"color:green;\">**Chargement des Données :**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c91c37d",
   "metadata": {},
   "source": [
    "#### <span style=\"color:orange;\">**1. Etablir la Connection avec MongoDB :**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "08fd6c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pymongo import MongoClient\n",
    "\n",
    "client = MongoClient(\"mongodb://localhost:27017/\")\n",
    "\n",
    "db = client[\"sparkattrinet\"]\n",
    "\n",
    "collection = db[\"bank\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7846f429",
   "metadata": {},
   "source": [
    "#### <span style=\"color:orange;\">**2. Créer la Session Spark :**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "66040604",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark =(\n",
    "    SparkSession.builder\n",
    "    .appName(\"SparkAttriNet - Prédiction de l'Attrition Client Bancaire\")\n",
    "    .config(\"spark.executor.memory\", \"8g\")\n",
    "    .config(\"spark.driver.memory\", \"8g\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc60ecd",
   "metadata": {},
   "source": [
    "#### <span style=\"color:orange;\">**3. Récupérer les Données et les Enregistrer sous format CSV :**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "eb7f9ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = list(collection.find()) \n",
    "\n",
    "for doc in docs :\n",
    "    doc[\"_id\"] = str(doc['_id'])\n",
    "\n",
    "pd.DataFrame(docs).to_csv(\"../data/temp/data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b6c725",
   "metadata": {},
   "source": [
    "#### <span style=\"color:orange;\">**4. Charger les Doonées sous format PySpark DataFrame :**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "3247ab03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Sex: double (nullable = true)\n",
      " |-- Region: double (nullable = true)\n",
      " |-- CreditScore: double (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Tenure: integer (nullable = true)\n",
      " |-- Balance: double (nullable = true)\n",
      " |-- NumOfProducts: integer (nullable = true)\n",
      " |-- HasCrCard: integer (nullable = true)\n",
      " |-- IsActiveMember: integer (nullable = true)\n",
      " |-- EstimatedSalary: double (nullable = true)\n",
      " |-- Exited: integer (nullable = true)\n",
      "\n",
      "+---+------+-----------+---+------+---------+-------------+---------+--------------+---------------+------+\n",
      "|Sex|Region|CreditScore|Age|Tenure|Balance  |NumOfProducts|HasCrCard|IsActiveMember|EstimatedSalary|Exited|\n",
      "+---+------+-----------+---+------+---------+-------------+---------+--------------+---------------+------+\n",
      "|1.0|0.0   |619.0      |42 |2     |0.0      |1            |1        |1             |101348.88      |1     |\n",
      "|1.0|2.0   |608.0      |41 |1     |83807.86 |1            |0        |1             |112542.58      |0     |\n",
      "|1.0|0.0   |502.0      |42 |8     |159660.8 |3            |1        |0             |113931.57      |1     |\n",
      "|1.0|0.0   |699.0      |39 |1     |0.0      |2            |0        |0             |93826.63       |0     |\n",
      "|1.0|2.0   |850.0      |43 |2     |125510.82|1            |1        |1             |79084.1        |0     |\n",
      "+---+------+-----------+---+------+---------+-------------+---------+--------------+---------------+------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\n",
    "    \"../data/temp/data.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "df = df.drop(\"_id\")\n",
    "\n",
    "df.printSchema()\n",
    "\n",
    "df.show(5,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67cc2cdc",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### <span style=\"color:green;\">**Gestion du Déséquilibre de Classes :**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757e8ba7",
   "metadata": {},
   "source": [
    "#### <span style=\"color:orange;\">**1. Identifier la Classe Majoritaire :**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "e61d0171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exited\n",
      "0    7959\n",
      "1    2037\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abdel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:95: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:\n",
      "  [PACKAGE_NOT_INSTALLED] PyArrow >= 11.0.0 must be installed; however, it was not found.\n",
      "Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "p_df = df.toPandas()\n",
    "\n",
    "print(p_df[\"Exited\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4e7ddb",
   "metadata": {},
   "source": [
    "#### <span style=\"color:orange;\">**2. Appliquer SMOTE Sampling :**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "570f56c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exited\n",
      "1    7959\n",
      "0    7959\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "X = p_df.drop(columns=[\"Exited\"])\n",
    "y = p_df[\"Exited\"]\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_res, y_res = smote.fit_resample(X, y)\n",
    "\n",
    "p_df_res = pd.concat([X_res, y_res], axis=1)\n",
    "\n",
    "print(p_df_res[\"Exited\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a490acde",
   "metadata": {},
   "source": [
    "#### <span style=\"color:orange;\">**3. Transformer en PySpark :**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "796f456d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|Exited|count|\n",
      "+------+-----+\n",
      "|     1| 7959|\n",
      "|     0| 7959|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "p_df_res.to_csv(\"../data/temp/data.csv\", index=False)\n",
    "\n",
    "df = spark.read.csv(\n",
    "    \"../data/temp/data.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "df.groupBy(\"Exited\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01cf437e",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### <span style=\"color:green;\">**Selection, Assemblage et Standarisation des Features :**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d249eac4",
   "metadata": {},
   "source": [
    "#### <span style=\"color:orange;\">**1. Sélectionner et Assembler les Features :**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "2080f86d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------+------+\n",
      "|features                                               |Exited|\n",
      "+-------------------------------------------------------+------+\n",
      "|[1.0,0.0,619.0,42.0,2.0,0.0,1.0,1.0,1.0,101348.88]     |1     |\n",
      "|[1.0,2.0,608.0,41.0,1.0,83807.86,1.0,0.0,1.0,112542.58]|0     |\n",
      "+-------------------------------------------------------+------+\n",
      "only showing top 2 rows\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "15918"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "feature_cols = [col for col in df.columns if col != \"Exited\"]\n",
    "\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "\n",
    "df_vec = assembler.transform(df)\n",
    "\n",
    "df_vec = df_vec.select(\"features\", \"Exited\")\n",
    "\n",
    "df_vec.show(2, truncate=False)\n",
    "\n",
    "df_vec.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734d69f1",
   "metadata": {},
   "source": [
    "#### <span style=\"color:orange;\">**2. Standariser les Features :**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "85aa36d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "scaler = StandardScaler(\n",
    "    inputCol=\"features\",\n",
    "    outputCol=\"scaled_features\",\n",
    "    withMean=True,\n",
    "    withStd=True\n",
    ")\n",
    "\n",
    "df_scaled = scaler.fit(df_vec).transform(df_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "26adbbb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------+\n",
      "|scaled_features                                                                                                                                                                                           |Exited|\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------+\n",
      "|[1.0732410343916199,-1.0189022035876123,-0.3294205389157971,0.09798913406328658,-1.039040761758242,-1.3342758555096281,-0.7320491812025733,0.7748578372390554,1.3051274493010556,0.01416412973324428]     |1     |\n",
      "|[1.0732410343916199,1.623344785208039,-0.44980039292599105,-0.0019142180163445482,-1.4063310916569278,0.030677462805769884,-0.7320491812025733,-1.2904782399831956,1.3051274493010556,0.20809304773685117]|0     |\n",
      "|[1.0732410343916199,-1.0189022035876123,-1.6098244406605875,0.09798913406328658,1.1647012176338718,1.2660714002834572,2.751585756430477,0.7748578372390554,-0.7661605604670605,0.23215705944145562]       |1     |\n",
      "|[1.0732410343916199,-1.0189022035876123,0.546069308431068,-0.2017209221756068,-1.4063310916569278,-1.3342758555096281,1.0097682876139518,-1.2904782399831956,-0.7661605604670605,-0.11615755152073182]    |0     |\n",
      "|[1.0732410343916199,1.623344785208039,2.1985563952982763,0.19789248614291768,-1.039040761758242,0.7098809842365083,-0.7320491812025733,0.7748578372390554,1.3051274493010556,-0.37156933596517067]        |0     |\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "df_scaled = df_scaled.drop(\"features\")\n",
    "\n",
    "df_scaled = df_scaled.select([\"scaled_features\", \"Exited\"])\n",
    "\n",
    "df_scaled.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0670e25f",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### <span style=\"color:green;\">**Séparation des Données et Choix du Modèle :**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129dac76",
   "metadata": {},
   "source": [
    "#### <span style=\"color:orange;\">**1. Séparation des Données :**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "0425d006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train count: 12797\n",
      "- Test count: 3121\n",
      "\n",
      "- Train Pourcentage : 80.39 %\n",
      "- Test Pourcentage : 19.61 %\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df = df_scaled.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "print(\"- Train count:\", train_df.count())\n",
    "print(\"- Test count:\", test_df.count())\n",
    "\n",
    "print(f\"\\n- Train Pourcentage : {(train_df.count() * 100)/df_scaled.count() :.2f} %\")\n",
    "print(f\"- Test Pourcentage : {(test_df.count() * 100)/df_scaled.count() :.2f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5e3e7f",
   "metadata": {},
   "source": [
    "#### <span style=\"color:orange;\">**2. Choisir le Modèle :**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d219f07",
   "metadata": {},
   "source": [
    "**Caractéristiques de Problématique :**\n",
    "\n",
    "- **Type de problème :** Classification binaire (Exited = 0 ou 1).\n",
    "\n",
    "- **Nombre de features :** 10 entre numériques et catégorielles encodées.\n",
    "\n",
    "- **Taille du dataset :** Presque 16 000 lignes après Equilibrage avec SMOTE.\n",
    "\n",
    "- Features standardisées (StandardScaler)\n",
    "\n",
    "**Logistic Regression :**\n",
    "\n",
    "- **Avantages :**\n",
    "\n",
    "    - Simple et rapide à entraîner\n",
    "\n",
    "    - Interprétable : coefficients permettent de comprendre l’importance de chaque feature\n",
    "\n",
    "    - Fonctionne très bien sur des features standardisées\n",
    "\n",
    "    - Bien adaptée aux datasets de taille moyenne (~10k–100k lignes)\n",
    "\n",
    "- **Limites :**\n",
    "\n",
    "    - Modèle linéaire → il ne capture pas les interactions complexes entre features\n",
    "\n",
    "    - Moins performant si les relations sont non linéaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "0c6d28e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(\n",
    "    featuresCol=\"scaled_features\",\n",
    "    labelCol=\"Exited\",\n",
    "    maxIter=100,\n",
    "    regParam=0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d6c02c",
   "metadata": {},
   "source": [
    "#### <span style=\"color:orange;\">**3. Enregistrer les Variables avec ``joblib`` :**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "3433b01b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abdel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:95: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:\n",
      "  [PACKAGE_NOT_INSTALLED] PyArrow >= 11.0.0 must be installed; however, it was not found.\n",
      "Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.\n",
      "  warn(msg)\n",
      "c:\\Users\\abdel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:95: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:\n",
      "  [PACKAGE_NOT_INSTALLED] PyArrow >= 11.0.0 must be installed; however, it was not found.\n",
      "Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.\n",
      "  warn(msg)\n",
      "c:\\Users\\abdel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:95: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:\n",
      "  [PACKAGE_NOT_INSTALLED] PyArrow >= 11.0.0 must be installed; however, it was not found.\n",
      "Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.\n",
      "  warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['../utils/test_df.pkl']"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "df_scaled_pd = df_scaled.toPandas()\n",
    "train_df_pd = train_df.toPandas()\n",
    "test_df_pd = test_df.toPandas()\n",
    "\n",
    "df_scaled_pd.to_csv(\"../data/scaled/data.csv\", index=False)\n",
    "\n",
    "joblib.dump(df_scaled_pd, \"../utils/df_scaled.pkl\")\n",
    "joblib.dump(train_df_pd, \"../utils/train_df.pkl\")\n",
    "joblib.dump(test_df_pd, \"../utils/test_df.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96729433",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### <span style=\"color:green;\">**Construction du Pipeline :**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "b0ed53f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "pipeline = Pipeline(stages=[assembler, scaler, lr]) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
